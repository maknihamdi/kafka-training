# Exercice 1 - Kafka Connect : Source et Sink Connectors

## ğŸ¯ Objectifs

- DÃ©ployer un cluster Kafka Connect
- Comprendre l'architecture Kafka Connect (Workers, Connectors, Tasks)
- CrÃ©er un JDBC Source Connector pour lire depuis PostgreSQL
- CrÃ©er un JDBC Sink Connector pour Ã©crire dans PostgreSQL
- Explorer les transformations (SMTs - Single Message Transforms)
- Monitorer les connecteurs via Kafka UI

## ğŸ“‹ PrÃ©requis

- Docker et Docker Compose installÃ©s
- curl et jq installÃ©s (pour les commandes API)
- Ports disponibles: 8080, 8083, 9092, 5432

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL         â”‚
â”‚  Table: source_data  â”‚
â”‚  (donnÃ©es source)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    JDBC Source Connector         â”‚
â”‚  (lit les nouvelles lignes)      â”‚
â”‚  Mode: incrementing (id)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Topic: db-source_data          â”‚
â”‚      (Kafka Broker)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     JDBC Sink Connector          â”‚
â”‚  (Ã©crit dans PostgreSQL)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL                     â”‚
â”‚  Table: sink_data                â”‚
â”‚  (donnÃ©es destination)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pipeline complet:** `source_data` â†’ `JDBC Source` â†’ `Kafka` â†’ `JDBC Sink` â†’ `sink_data`

---

## ğŸš€ Partie 1 - DÃ©marrage de l'Infrastructure

### 1.1 DÃ©marrer les services

```bash
cd jour2/exercice1-kafka-connect
make start
```

Ou directement avec docker-compose:

```bash
docker compose up -d
```

**Services dÃ©marrÃ©s:**
- Kafka Broker (port 9092)
- Kafka Connect (port 8083) avec JDBC Connector installÃ©
- PostgreSQL (port 5432)
- Kafka UI (port 8080)

**â³ Temps d'attente:** ~90 secondes pour l'installation du JDBC Connector

### 1.2 VÃ©rifier le statut

```bash
make status
```

Attendez que tous les services soient "healthy".

### 1.3 AccÃ©der aux interfaces

**Kafka UI:** http://localhost:8080
- Topics
- Consumer Groups
- **Kafka Connect** (menu Ã  gauche)

**Kafka Connect API:** http://localhost:8083

Tester l'API:
```bash
curl http://localhost:8083/
```

**PostgreSQL:**
```bash
make db-connect
```

Ou directement:
```bash
docker exec -it postgres psql -U kafka -d kafka_sink
```

---

## ğŸ“Š Partie 2 - Explorer les DonnÃ©es Initiales

### 2.1 VÃ©rifier la table source

Les donnÃ©es sont initialisÃ©es automatiquement au dÃ©marrage de PostgreSQL via le script `init-db.sql`.

```bash
make db-query
```

Ou manuellement:
```bash
docker exec -it postgres psql -U kafka -d kafka_sink -c "SELECT * FROM source_data;"
```

**RÃ©sultat attendu:**
```
 id |           message            |         created_at
----+------------------------------+----------------------------
  1 | Hello Kafka Connect!         | 2025-12-01 14:30:00.123456
  2 | This is message number 2     | 2025-12-01 14:30:00.234567
  3 | Kafka Connect is awesome     | 2025-12-01 14:30:00.345678
  4 | Learning Kafka is fun        | 2025-12-01 14:30:00.456789
  5 | Data integration made easy   | 2025-12-01 14:30:00.567890
```

### 2.2 VÃ©rifier la table destination (vide au dÃ©part)

```bash
docker exec -it postgres psql -U kafka -d kafka_sink -c "SELECT * FROM sink_data;"
```

**RÃ©sultat attendu:** Aucune ligne (la table est vide)

### 2.3 Structure des tables

**Table source_data:**
- `id` (SERIAL PRIMARY KEY) - Identifiant auto-incrÃ©mentÃ©
- `message` (TEXT) - Contenu du message
- `created_at` (TIMESTAMP) - Date de crÃ©ation

**Table sink_data:**
- `id` (INT PRIMARY KEY) - Identifiant
- `message` (TEXT) - Contenu du message
- `created_at` (TIMESTAMP) - Date de crÃ©ation

---

## ğŸ“¥ Partie 3 - Source Connector (Lire depuis PostgreSQL)

### 3.1 Comprendre le Source Connector

Le **JDBC Source Connector** lit les donnÃ©es depuis une table PostgreSQL et les envoie vers Kafka.

**Mode incrementing:**
- Lit uniquement les nouvelles lignes
- Utilise la colonne `id` pour tracker la progression
- Poll interval: 5 secondes

### 3.2 CrÃ©er le Source Connector

**Via Make:**
```bash
make create-source
```

**Via API REST:**
```bash
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d '{
    "name": "jdbc-source",
    "config": {
      "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
      "tasks.max": "1",
      "connection.url": "jdbc:postgresql://postgres:5432/kafka_sink",
      "connection.user": "kafka",
      "connection.password": "kafka123",
      "table.whitelist": "source_data",
      "mode": "incrementing",
      "incrementing.column.name": "id",
      "topic.prefix": "db-",
      "poll.interval.ms": "5000"
    }
  }'
```

**Configuration expliquÃ©e:**
- `table.whitelist`: Table(s) Ã  lire
- `mode: incrementing`: Lit les nouvelles lignes basÃ©es sur l'ID
- `incrementing.column.name: id`: Colonne utilisÃ©e pour tracker
- `topic.prefix: db-`: PrÃ©fixe du topic (â†’ `db-source_data`)
- `poll.interval.ms: 5000`: VÃ©rifie les nouvelles donnÃ©es toutes les 5s

### 3.3 VÃ©rifier le connector

```bash
make connectors
```

Ou via API:
```bash
curl http://localhost:8083/connectors/jdbc-source/status | jq
```

**Statut attendu:**
```json
{
  "name": "jdbc-source",
  "connector": {
    "state": "RUNNING",
    "worker_id": "kafka-connect:8083"
  },
  "tasks": [
    {
      "id": 0,
      "state": "RUNNING",
      "worker_id": "kafka-connect:8083"
    }
  ],
  "type": "source"
}
```

### 3.4 Observer le rÃ©sultat

**VÃ©rifier le topic Kafka:**
```bash
make topics
```

**Consommer le topic pour voir les messages:**
```bash
make consume
```

Ou manuellement:
```bash
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic db-source_data \
  --from-beginning \
  --max-messages 5
```

**Dans Kafka UI:**
1. Allez dans "Topics" â†’ "db-source_data"
2. Cliquez sur "Messages"
3. Observez les 5 messages initiaux

---

## ğŸ“¤ Partie 4 - Sink Connector (Ã‰crire dans PostgreSQL)

### 4.1 Comprendre le Sink Connector

Le **JDBC Sink Connector** lit les messages depuis Kafka et les Ã©crit dans PostgreSQL.

**Configuration:**
- `insert.mode: insert`: InsÃ¨re les nouvelles lignes
- `pk.mode: record_value`: Utilise l'ID du message comme clÃ© primaire
- `pk.fields: id`: Champ utilisÃ© comme clÃ© primaire

### 4.2 CrÃ©er le Sink Connector

**Via Make:**
```bash
make create-sink
```

**Via API REST:**
```bash
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d '{
    "name": "jdbc-sink",
    "config": {
      "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
      "tasks.max": "1",
      "topics": "db-source_data",
      "connection.url": "jdbc:postgresql://postgres:5432/kafka_sink",
      "connection.user": "kafka",
      "connection.password": "kafka123",
      "auto.create": "false",
      "auto.evolve": "false",
      "insert.mode": "insert",
      "table.name.format": "sink_data",
      "pk.mode": "record_value",
      "pk.fields": "id"
    }
  }'
```

### 4.3 VÃ©rifier le connector

```bash
make connectors
```

### 4.4 VÃ©rifier les donnÃ©es dans PostgreSQL

```bash
make db-query
```

Ou manuellement:
```bash
docker exec -it postgres psql -U kafka -d kafka_sink -c "SELECT * FROM sink_data ORDER BY id;"
```

**RÃ©sultat attendu:** Les 5 messages de `source_data` sont maintenant dans `sink_data`!

---

## ğŸ”„ Partie 5 - Test du Pipeline Complet

### 5.1 Ajouter une nouvelle donnÃ©e

**Via Make:**
```bash
make add-data
```

**Via PostgreSQL directement:**
```bash
docker exec -it postgres psql -U kafka -d kafka_sink -c \
  "INSERT INTO source_data (message) VALUES ('New message from Kafka training');"
```

### 5.2 Observer le traitement

**Ã‰tape 1: VÃ©rifier la table source**
```bash
make db-query
```

Vous devriez voir le nouveau message avec un ID supÃ©rieur (ex: id=6).

**Ã‰tape 2: Attendre 5 secondes** (poll interval du source connector)

**Ã‰tape 3: VÃ©rifier le topic Kafka**
```bash
make consume
```

Vous devriez voir le nouveau message dans le topic.

**Ã‰tape 4: VÃ©rifier la table destination**
```bash
make db-query
```

Vous devriez voir le nouveau message dans `sink_data`!

### 5.3 Ajouter plusieurs messages

```bash
docker exec -it postgres psql -U kafka -d kafka_sink << EOF
INSERT INTO source_data (message) VALUES
  ('Message 1'),
  ('Message 2'),
  ('Message 3');
EOF
```

Attendez 5 secondes et vÃ©rifiez:
```bash
make db-query
```

**Tous les messages** ont Ã©tÃ© propagÃ©s automatiquement!

---

## ğŸ”§ Partie 6 - Transformations (SMTs)

### 6.1 Comprendre les transformations

Les **Single Message Transforms (SMTs)** permettent de modifier les messages Ã  la volÃ©e dans Kafka Connect, sans avoir besoin d'Ã©crire de code.

**Cas d'usage courants:**
- Ajouter des mÃ©tadonnÃ©es (timestamp, source, environnement)
- Renommer ou supprimer des champs
- Masquer des donnÃ©es sensibles (RGPD, sÃ©curitÃ©)
- Convertir les types de donnÃ©es
- Modifier le format des timestamps

**Types de transformations disponibles:**
- **InsertField** - Ajouter un champ (ex: timestamp, hostname)
- **ReplaceField** - Renommer ou supprimer des champs
- **MaskField** - Masquer des donnÃ©es sensibles
- **Cast** - Convertir les types de donnÃ©es
- **TimestampConverter** - Convertir les formats de temps
- **ValueToKey** - Copier un champ de la valeur vers la clÃ©

---

### 6.2 Exemple GuidÃ© : Ajouter des MÃ©tadonnÃ©es

Dans cet exemple, nous allons enrichir chaque message avec :
1. Un champ `source_system` pour identifier l'origine des donnÃ©es
2. Un timestamp `processed_at` pour tracer quand le message a Ã©tÃ© traitÃ©
3. Un champ `environment` pour identifier l'environnement (dev/prod)

#### Ã‰tape 1 : Supprimer le connector actuel

```bash
make delete-source
```

#### Ã‰tape 2 : CrÃ©er le connector avec transformations

```bash
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d '{
    "name": "jdbc-source",
    "config": {
      "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
      "tasks.max": "1",
      "connection.url": "jdbc:postgresql://postgres:5432/kafka_sink",
      "connection.user": "kafka",
      "connection.password": "kafka123",
      "table.whitelist": "source_data",
      "mode": "incrementing",
      "incrementing.column.name": "id",
      "topic.prefix": "db-",
      "poll.interval.ms": "5000",
      "transforms": "AddSource,AddTimestamp,AddEnv",
      "transforms.AddSource.type": "org.apache.kafka.connect.transforms.InsertField$Value",
      "transforms.AddSource.static.field": "source_system",
      "transforms.AddSource.static.value": "postgresql-prod",
      "transforms.AddTimestamp.type": "org.apache.kafka.connect.transforms.InsertField$Value",
      "transforms.AddTimestamp.timestamp.field": "processed_at",
      "transforms.AddEnv.type": "org.apache.kafka.connect.transforms.InsertField$Value",
      "transforms.AddEnv.static.field": "environment",
      "transforms.AddEnv.static.value": "training"
    }
  }'
```

**Explication de la configuration :**
- `transforms`: Liste des transformations Ã  appliquer (ordre d'exÃ©cution)
- `AddSource`: Ajoute un champ statique `source_system = "postgresql-prod"`
- `AddTimestamp`: Ajoute un timestamp automatique `processed_at`
- `AddEnv`: Ajoute un champ statique `environment = "training"`

#### Ã‰tape 3 : VÃ©rifier le connector

```bash
curl http://localhost:8083/connectors/jdbc-source/status | jq
```

Attendez que le statut soit `RUNNING`.

#### Ã‰tape 4 : Ajouter une nouvelle donnÃ©e

```bash
make add-data
```

Ou manuellement :
```bash
docker exec -it postgres psql -U kafka -d kafka_sink -c \
  "INSERT INTO source_data (message) VALUES ('Test avec transformations SMT');"
```

#### Ã‰tape 5 : Observer le rÃ©sultat transformÃ©

**Dans le terminal :**
```bash
make consume
```

**RÃ©sultat attendu (format Struct) :**
```
Struct{
  id=10,
  message=Test avec transformations SMT,
  created_at=2025-12-01 16:30:00.123456,
  source_system=postgresql-prod,
  processed_at=1733071800000,
  environment=training
}
```

**Dans Kafka UI :**
1. Allez dans "Topics" â†’ "db-source_data"
2. Cliquez sur "Messages"
3. SÃ©lectionnez le dernier message
4. Observez les nouveaux champs ajoutÃ©s par les transformations !

#### Ã‰tape 6 : Comparer avec les donnÃ©es originales

```bash
make db-query
```

Vous constaterez que la table source PostgreSQL ne contient que les 3 champs originaux (`id`, `message`, `created_at`), tandis que le message Kafka contient 6 champs grÃ¢ce aux transformations !

---

### 6.3 Exercice Pratique : Renommer des Champs

**Objectif :** Renommer `message` â†’ `content` et `created_at` â†’ `timestamp`

#### Configuration Ã  ajouter

Supprimez le connector et recrÃ©ez-le avec cette transformation supplÃ©mentaire :

```bash
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d '{
    "name": "jdbc-source",
    "config": {
      "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
      "tasks.max": "1",
      "connection.url": "jdbc:postgresql://postgres:5432/kafka_sink",
      "connection.user": "kafka",
      "connection.password": "kafka123",
      "table.whitelist": "source_data",
      "mode": "incrementing",
      "incrementing.column.name": "id",
      "topic.prefix": "db-",
      "poll.interval.ms": "5000",
      "transforms": "RenameFields",
      "transforms.RenameFields.type": "org.apache.kafka.connect.transforms.ReplaceField$Value",
      "transforms.RenameFields.renames": "message:content,created_at:timestamp"
    }
  }'
```

**Testez :**
1. Ajoutez une nouvelle donnÃ©e : `make add-data`
2. Consommez le topic : `make consume`
3. VÃ©rifiez que les champs sont renommÃ©s !

**RÃ©sultat attendu :**
```
Struct{
  id=11,
  content=...,          â† RenommÃ© de "message"
  timestamp=...         â† RenommÃ© de "created_at"
}
```

---

### 6.4 Exercice Pratique : ChaÃ®ner Plusieurs Transformations

**Objectif :** Combiner renommage + ajout de mÃ©tadonnÃ©es

```bash
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d '{
    "name": "jdbc-source",
    "config": {
      "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
      "tasks.max": "1",
      "connection.url": "jdbc:postgresql://postgres:5432/kafka_sink",
      "connection.user": "kafka",
      "connection.password": "kafka123",
      "table.whitelist": "source_data",
      "mode": "incrementing",
      "incrementing.column.name": "id",
      "topic.prefix": "db-",
      "poll.interval.ms": "5000",
      "transforms": "RenameFields,AddMetadata,AddTimestamp",
      "transforms.RenameFields.type": "org.apache.kafka.connect.transforms.ReplaceField$Value",
      "transforms.RenameFields.renames": "message:content",
      "transforms.AddMetadata.type": "org.apache.kafka.connect.transforms.InsertField$Value",
      "transforms.AddMetadata.static.field": "data_source",
      "transforms.AddMetadata.static.value": "postgres-training-db",
      "transforms.AddTimestamp.type": "org.apache.kafka.connect.transforms.InsertField$Value",
      "transforms.AddTimestamp.timestamp.field": "ingestion_time"
    }
  }'
```

**âš ï¸ Important :** Les transformations sont appliquÃ©es dans l'ordre :
1. D'abord `RenameFields` (renomme `message` â†’ `content`)
2. Ensuite `AddMetadata` (ajoute `data_source`)
3. Enfin `AddTimestamp` (ajoute `ingestion_time`)

**Testez et observez le rÃ©sultat !**

---

### 6.5 Autres Transformations Utiles

#### Masquer des DonnÃ©es Sensibles (MaskField)

**Cas d'usage :** RGPD, sÃ©curitÃ©, logs

```json
{
  "transforms": "MaskSensitive",
  "transforms.MaskSensitive.type": "org.apache.kafka.connect.transforms.MaskField$Value",
  "transforms.MaskSensitive.fields": "password,ssn,credit_card",
  "transforms.MaskSensitive.replacement": "****MASKED****"
}
```

Les champs `password`, `ssn`, et `credit_card` seront remplacÃ©s par `****MASKED****`.

#### Convertir les Types (Cast)

**Cas d'usage :** Forcer un type de donnÃ©es

```json
{
  "transforms": "CastTypes",
  "transforms.CastTypes.type": "org.apache.kafka.connect.transforms.Cast$Value",
  "transforms.CastTypes.spec": "age:int32,price:float64,active:boolean"
}
```

#### Supprimer des Champs (ReplaceField)

**Cas d'usage :** RÃ©duire la taille des messages, supprimer des donnÃ©es inutiles

```json
{
  "transforms": "DropFields",
  "transforms.DropFields.type": "org.apache.kafka.connect.transforms.ReplaceField$Value",
  "transforms.DropFields.blacklist": "internal_id,debug_info,temp_field"
}
```

#### Convertir Format de Timestamp (TimestampConverter)

**Cas d'usage :** Convertir epoch â†’ ISO 8601

```json
{
  "transforms": "ConvertTimestamp",
  "transforms.ConvertTimestamp.type": "org.apache.kafka.connect.transforms.TimestampConverter$Value",
  "transforms.ConvertTimestamp.field": "created_at",
  "transforms.ConvertTimestamp.format": "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'",
  "transforms.ConvertTimestamp.target.type": "string"
}
```

---

### 6.6 ğŸ“ DÃ©fi : CrÃ©er Votre Propre Pipeline de Transformations

**Objectif :** CrÃ©ez un connector avec les transformations suivantes :

1. Renommer `message` â†’ `event_description`
2. Ajouter un champ `pipeline_version` avec la valeur `"v1.0"`
3. Ajouter un timestamp `transformed_at`
4. Supprimer le champ `created_at` (si vous ne le voulez pas)

**Indice :** Utilisez `ReplaceField` avec `renames` et `blacklist`, ainsi que plusieurs `InsertField`.

**Solution :**
<details>
<summary>Cliquez pour voir la solution</summary>

```bash
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d '{
    "name": "jdbc-source",
    "config": {
      "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
      "tasks.max": "1",
      "connection.url": "jdbc:postgresql://postgres:5432/kafka_sink",
      "connection.user": "kafka",
      "connection.password": "kafka123",
      "table.whitelist": "source_data",
      "mode": "incrementing",
      "incrementing.column.name": "id",
      "topic.prefix": "db-",
      "poll.interval.ms": "5000",
      "transforms": "RenameAndDrop,AddVersion,AddTimestamp",
      "transforms.RenameAndDrop.type": "org.apache.kafka.connect.transforms.ReplaceField$Value",
      "transforms.RenameAndDrop.renames": "message:event_description",
      "transforms.RenameAndDrop.blacklist": "created_at",
      "transforms.AddVersion.type": "org.apache.kafka.connect.transforms.InsertField$Value",
      "transforms.AddVersion.static.field": "pipeline_version",
      "transforms.AddVersion.static.value": "v1.0",
      "transforms.AddTimestamp.type": "org.apache.kafka.connect.transforms.InsertField$Value",
      "transforms.AddTimestamp.timestamp.field": "transformed_at"
    }
  }'
```
</details>

**Testez votre solution et vÃ©rifiez le rÃ©sultat avec `make consume` !**

---

## ğŸ“Š Partie 7 - Monitoring et Debugging

### 7.1 VÃ©rifier l'Ã©tat des connecteurs

**Via Kafka UI:**
1. Menu "Kafka Connect"
2. Voir la liste des connecteurs
3. Cliquer sur un connecteur pour voir les dÃ©tails
4. Observer les mÃ©triques (messages traitÃ©s, erreurs, etc.)

**Via API:**
```bash
# Statut d'un connector
curl http://localhost:8083/connectors/jdbc-source/status | jq

# Configuration d'un connector
curl http://localhost:8083/connectors/jdbc-source/config | jq

# Tasks d'un connector
curl http://localhost:8083/connectors/jdbc-source/tasks | jq
```

### 7.2 Logs des connecteurs

```bash
# Logs de Kafka Connect
docker logs kafka-connect -f

# Filtrer les erreurs
docker logs kafka-connect 2>&1 | grep ERROR

# Logs d'un connecteur spÃ©cifique
docker logs kafka-connect 2>&1 | grep jdbc-source
```

### 7.3 RedÃ©marrer un connector en cas de problÃ¨me

**Via Make:**
```bash
make delete-source
make create-source
```

**Via API:**
```bash
curl -X POST http://localhost:8083/connectors/jdbc-source/restart
```

### 7.4 MÃ©triques importantes

**Offset tracking:**
```bash
curl http://localhost:8083/connectors/jdbc-source/status | jq '.tasks[0]'
```

**Topic lag:**
```bash
docker exec kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 \
  --describe \
  --group connect-jdbc-sink
```

### 7.5 ProblÃ¨mes courants

**Le connector ne dÃ©marre pas:**
- VÃ©rifiez les logs: `docker logs kafka-connect`
- VÃ©rifiez la configuration: `curl http://localhost:8083/connectors/<name>/config`
- VÃ©rifiez la connexion Ã  PostgreSQL

**Les nouvelles donnÃ©es ne sont pas dÃ©tectÃ©es:**
- VÃ©rifiez le poll interval (5 secondes par dÃ©faut)
- VÃ©rifiez que l'ID est bien incrÃ©mentÃ©
- VÃ©rifiez les logs du source connector

**Les donnÃ©es n'apparaissent pas dans sink_data:**
- VÃ©rifiez que le topic contient des messages: `make consume`
- VÃ©rifiez les logs du sink connector
- VÃ©rifiez la connexion Ã  PostgreSQL
- VÃ©rifiez que les IDs ne sont pas dupliquÃ©s (erreur de PK)

---

## ğŸ¯ Livrables

Ã€ la fin de cet exercice, vous devez Ãªtre capable de:

### Infrastructure
- [ ] DÃ©ployer un cluster Kafka Connect avec Docker Compose
- [ ] VÃ©rifier l'Ã©tat des services (Kafka, Connect, PostgreSQL)
- [ ] AccÃ©der Ã  Kafka UI et Ã  l'API Connect

### Source Connector
- [ ] CrÃ©er un JDBC Source Connector
- [ ] Comprendre le mode incrementing
- [ ] VÃ©rifier que les donnÃ©es sont lues depuis PostgreSQL
- [ ] Observer les messages dans le topic Kafka

### Sink Connector
- [ ] CrÃ©er un JDBC Sink Connector
- [ ] Configurer la connexion Ã  PostgreSQL
- [ ] VÃ©rifier que les donnÃ©es sont Ã©crites dans la base
- [ ] RequÃªter la table sink_data pour voir les rÃ©sultats

### Pipeline Complet
- [ ] Ajouter des donnÃ©es dans source_data
- [ ] Observer la propagation automatique
- [ ] VÃ©rifier que les donnÃ©es arrivent dans sink_data
- [ ] Comprendre le flux end-to-end

### Transformations
- [ ] Comprendre le concept de SMT
- [ ] Ajouter une transformation InsertField
- [ ] ChaÃ®ner plusieurs transformations
- [ ] Observer l'impact des transformations sur les donnÃ©es

### Monitoring
- [ ] Utiliser Kafka UI pour monitorer les connecteurs
- [ ] Utiliser l'API REST pour vÃ©rifier l'Ã©tat
- [ ] Lire les logs pour dÃ©bugger
- [ ] RedÃ©marrer un connector en cas de problÃ¨me

---

## ğŸ§¹ Nettoyage

### ArrÃªter les services

```bash
make stop
```

### Nettoyage complet

```bash
make clean
```

---

## ğŸ“š Concepts ClÃ©s

### Kafka Connect Architecture

- **Worker** - Processus qui exÃ©cute les connecteurs
- **Connector** - Plugin qui dÃ©finit la logique de transfert de donnÃ©es
- **Task** - UnitÃ© de travail parallÃ©lisable (configurÃ© par `tasks.max`)
- **Converter** - Convertit les donnÃ©es entre Kafka et le format du connecteur

### Types de Connecteurs

**Source Connector:**
- Lit les donnÃ©es depuis un systÃ¨me externe
- Produit des messages vers Kafka
- Exemples: JDBC, Debezium (CDC), HTTP, S3

**Sink Connector:**
- Lit les messages depuis Kafka
- Ã‰crit les donnÃ©es vers un systÃ¨me externe
- Exemples: JDBC, Elasticsearch, S3, HDFS

### JDBC Source Connector - Modes

**Incrementing Mode:**
- Utilise une colonne auto-incrÃ©mentÃ©e (ex: ID)
- Lit uniquement les nouvelles lignes
- Ne dÃ©tecte PAS les mises Ã  jour ou suppressions
- Simple et performant

**Timestamp Mode:**
- Utilise une colonne timestamp (ex: updated_at)
- DÃ©tecte les nouvelles lignes et les mises Ã  jour
- Ne dÃ©tecte PAS les suppressions

**Timestamp+Incrementing Mode:**
- Combine les deux approches
- Plus robuste mais plus complexe

**Bulk Mode:**
- Lit toutes les lignes Ã  chaque poll
- Utilise beaucoup de ressources
- Ã€ Ã©viter en production

### Single Message Transforms (SMT)

**InsertField** - Ajouter un champ statique ou dynamique
```json
"transforms": "InsertTimestamp",
"transforms.InsertTimestamp.type": "org.apache.kafka.connect.transforms.InsertField$Value",
"transforms.InsertTimestamp.timestamp.field": "processed_at"
```

**ReplaceField** - Renommer ou exclure des champs
```json
"transforms": "RenameField",
"transforms.RenameField.type": "org.apache.kafka.connect.transforms.ReplaceField$Value",
"transforms.RenameField.renames": "old_name:new_name"
```

**MaskField** - Masquer des donnÃ©es sensibles
```json
"transforms": "MaskPassword",
"transforms.MaskPassword.type": "org.apache.kafka.connect.transforms.MaskField$Value",
"transforms.MaskPassword.fields": "password,ssn"
```

**Cast** - Convertir les types
```json
"transforms": "Cast",
"transforms.Cast.type": "org.apache.kafka.connect.transforms.Cast$Value",
"transforms.Cast.spec": "age:int32,price:float64"
```

### API REST Kafka Connect

| Endpoint | MÃ©thode | Description |
|----------|---------|-------------|
| `/connectors` | GET | Lister tous les connecteurs |
| `/connectors` | POST | CrÃ©er un nouveau connecteur |
| `/connectors/{name}` | GET | Obtenir les infos d'un connecteur |
| `/connectors/{name}/config` | PUT | Mettre Ã  jour la config |
| `/connectors/{name}/status` | GET | Statut du connecteur |
| `/connectors/{name}/restart` | POST | RedÃ©marrer le connecteur |
| `/connectors/{name}` | DELETE | Supprimer le connecteur |
| `/connector-plugins` | GET | Lister les plugins installÃ©s |

---

## ğŸ” Pour Aller Plus Loin

### 1. Change Data Capture (CDC) avec Debezium

Capturez les changements de base de donnÃ©es en temps rÃ©el:
```json
{
  "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
  "database.hostname": "postgres",
  "database.port": "5432",
  "database.user": "kafka",
  "database.password": "kafka123",
  "database.dbname": "kafka_sink",
  "database.server.name": "postgres-cdc",
  "table.include.list": "public.source_data"
}
```

### 2. Schema Registry avec Avro

Utilisez Schema Registry pour gÃ©rer les schÃ©mas:
```json
{
  "key.converter": "io.confluent.connect.avro.AvroConverter",
  "value.converter": "io.confluent.connect.avro.AvroConverter",
  "key.converter.schema.registry.url": "http://schema-registry:8081",
  "value.converter.schema.registry.url": "http://schema-registry:8081"
}
```

### 3. Dead Letter Queue (DLQ)

GÃ©rez les erreurs en redirigeant les messages problÃ©matiques:
```json
{
  "errors.tolerance": "all",
  "errors.deadletterqueue.topic.name": "dlq-topic",
  "errors.deadletterqueue.topic.replication.factor": "1",
  "errors.deadletterqueue.context.headers.enable": "true"
}
```

### 4. Distributed Mode en Production

En production, dÃ©ployez Kafka Connect en mode distribuÃ© avec plusieurs workers pour la haute disponibilitÃ© et la scalabilitÃ© horizontale.

---

**Bravo! Vous maÃ®trisez maintenant Kafka Connect et les transformations! ğŸš€**
