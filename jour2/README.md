# Jour 2 - Kafka Connect et Stream Processing

## üéØ Objectifs du Jour 2

Ce deuxi√®me jour se concentre sur trois outils essentiels de l'√©cosyst√®me Kafka:
- **Kafka Connect** - Pour l'int√©gration de donn√©es avec des syst√®mes externes
- **ksqlDB** - Pour le traitement en temps r√©el avec SQL
- **Kafka Streams** - Pour le stream processing avec Java/Spring Boot

√Ä la fin de ce jour, vous serez capables de:
- Cr√©er des pipelines d'int√©gration de donn√©es avec Kafka Connect
- Transformer les donn√©es √† la vol√©e avec les SMTs
- Utiliser ksqlDB pour filtrer, transformer et agr√©ger des donn√©es
- D√©velopper des applications Kafka Streams avec Spring Boot
- Comprendre et utiliser les topics compact√©s
- Impl√©menter des fen√™tres temporelles et des agr√©gations
- Faire des jointures entre streams et tables

## üìö Contenu

### [Exercice 1 - Kafka Connect : Source et Sink Connectors](./exercice1-kafka-connect/)

**Dur√©e estim√©e:** 2-3 heures

#### Objectifs
- Comprendre l'architecture Kafka Connect (Workers, Connectors, Tasks)
- D√©ployer un cluster Kafka Connect
- Cr√©er un Source Connector pour lire depuis PostgreSQL
- Cr√©er un Sink Connector pour √©crire dans PostgreSQL
- Appliquer des transformations (SMTs) aux messages

#### Ce que vous allez construire

```
PostgreSQL (source_data)
    ‚Üí JDBC Source Connector
    ‚Üí Kafka Topic
    ‚Üí JDBC Sink Connector
    ‚Üí PostgreSQL (sink_data)
```

#### Comp√©tences acquises
- Configuration et d√©ploiement de Kafka Connect
- Cr√©ation de connecteurs JDBC
- Application de SMTs (Single Message Transforms):
  - InsertField - Ajouter des m√©tadonn√©es
  - ReplaceField - Renommer/supprimer des champs
  - MaskField - Masquer des donn√©es sensibles
  - Cast - Convertir les types
- Monitoring via Kafka UI
- Debugging de connecteurs

#### Technologies
- Kafka Connect (Confluent Platform)
- JDBC Source/Sink Connectors
- PostgreSQL
- Kafka UI
- Docker Compose

---

### [Exercice 2 - ksqlDB : Stream Processing SQL](./exercice2-ksqldb/)

**Dur√©e estim√©e:** 2-3 heures

#### Objectifs
- Comprendre la diff√©rence entre STREAMS et TABLES
- Utiliser SQL pour traiter des flux de donn√©es en temps r√©el
- Impl√©menter des agr√©gations et des fen√™tres temporelles
- Joindre des streams et des tables
- Cr√©er des topics compact√©s

#### Ce que vous allez construire

```
Topic source (user_events)
    ‚Üí Streams (filtres, transformations)
    ‚Üí Tables (agr√©gations, √©tats)
    ‚Üí Windowing (fen√™tres temporelles)
    ‚Üí Joins (enrichissement)
    ‚Üí Topics d√©riv√©s
```

#### Comp√©tences acquises
- **Streams:**
  - Cr√©er des streams sur des topics existants
  - Filtrer les √©v√©nements (WHERE)
  - Transformer les donn√©es (CASE, colonnes calcul√©es)
  - Cr√©er des streams d√©riv√©s

- **Tables:**
  - Cr√©er des tables avec agr√©gations (COUNT, SUM, AVG, MAX, MIN)
  - Utiliser LATEST_BY_OFFSET pour l'√©tat actuel
  - Comprendre les topics compact√©s
  - Voir les mises √† jour en temps r√©el

- **Windowing:**
  - Fen√™tres TUMBLING (fixes)
  - Fen√™tres HOPPING (chevauchantes)
  - Fen√™tres SESSION (bas√©es sur l'activit√©)
  - Analyser les donn√©es par fen√™tre temporelle

- **Joins:**
  - Stream-Table Join (enrichir avec l'√©tat actuel)
  - Corr√©lation de donn√©es
  - D√©tection d'anomalies

#### Technologies
- ksqlDB Server et CLI
- Kafka UI avec support ksqlDB
- Shell script pour g√©n√©ration de donn√©es

---

### [Exercice 3 - Kafka Streams avec Spring Boot](./exercice3-kafka-streams/)

**Dur√©e estim√©e:** 3-4 heures

#### Objectifs
- Comprendre les concepts de Kafka Streams
- Utiliser l'API Admin de Kafka pour cr√©er des topics
- Cr√©er une pipeline de stream processing avec Spring Boot
- Manipuler KStream et KTable
- Faire des jointures entre streams et tables
- Utiliser des topics compact√©s pour les donn√©es de r√©f√©rence
- Visualiser la topologie Kafka Streams

#### Ce que vous allez construire

```
Producer (Spring Boot + API Admin)
    ‚Üì
    ‚îú‚îÄ> user-events (topic normal)
    ‚îî‚îÄ> user-profiles (topic compact√©)

Kafka Streams (Spring Boot)
    ‚Üì
    Filtre, Transformations, Agr√©gations
    ‚Üì
    Jointures avec KTable
    ‚Üì
    Topics de sortie enrichis
```

#### Comp√©tences acquises
- **API Admin Kafka:**
  - Cr√©er des topics programmatiquement
  - Configurer le compactage
  - G√©rer les partitions

- **Kafka Streams:**
  - KStream vs KTable vs GlobalKTable
  - Transformations (filter, map, flatMap)
  - Agr√©gations (count, sum, aggregate)
  - Fen√™tres temporelles (tumbling, hopping)
  - Jointures Stream-Table
  - Visualiser la topologie

- **Spring Boot:**
  - Configuration Kafka Streams
  - Serdes JSON personnalis√©s
  - Int√©gration avec Spring Kafka
  - REST API pour injection de donn√©es

#### Technologies
- Spring Boot 3.2
- Spring Kafka & Kafka Streams
- Maven multi-modules
- Kafka UI
- Docker Compose

---

## üöÄ D√©marrage Rapide

### Exercice 1 - Kafka Connect

```bash
cd jour2/exercice1-kafka-connect
make start
make create-source
make create-sink
make db-query
```

Ouvrez http://localhost:8080 pour Kafka UI.

### Exercice 2 - ksqlDB

```bash
cd jour2/exercice2-ksqldb
make start
make generate
make ksql-setup
make ksql
```

Dans le CLI ksqlDB, commencez √† cr√©er vos streams et tables!

### Exercice 3 - Kafka Streams

```bash
cd jour2/exercice3-kafka-streams
make start
make build
make run-producer   # Terminal 1
make run-streams    # Terminal 2
make init-data
make generate
```

Observez le traitement en temps r√©el dans les logs!

---

## üìñ Concepts Cl√©s du Jour 2

### Kafka Connect

**Kafka Connect** est un framework pour connecter Kafka avec des syst√®mes externes (bases de donn√©es, syst√®mes de fichiers, APIs, etc.).

**Architecture:**
- **Worker** - Processus qui ex√©cute les connecteurs
- **Connector** - Plugin qui d√©finit la logique de transfert
- **Task** - Unit√© de travail parall√©lisable
- **Converter** - Convertit les donn√©es entre Kafka et le format du connecteur

**Types de connecteurs:**
- **Source Connector** - Lit depuis un syst√®me externe ‚Üí Kafka
- **Sink Connector** - Lit depuis Kafka ‚Üí syst√®me externe

**Modes:**
- **Standalone** - Un seul worker (d√©veloppement, tests)
- **Distributed** - Plusieurs workers (production, haute disponibilit√©)

### SMTs (Single Message Transforms)

Les **SMTs** permettent de modifier les messages √† la vol√©e dans Kafka Connect, sans √©crire de code.

**Cas d'usage:**
- Ajouter des m√©tadonn√©es (timestamp, source, environnement)
- Renommer ou supprimer des champs
- Masquer des donn√©es sensibles (RGPD)
- Convertir les types de donn√©es
- Filtrer les messages

**SMTs courants:**
- `InsertField` - Ajouter un champ
- `ReplaceField` - Renommer/supprimer des champs
- `MaskField` - Masquer des donn√©es sensibles
- `Cast` - Convertir les types
- `TimestampConverter` - Convertir les formats de temps
- `ValueToKey` - Copier un champ de la valeur vers la cl√©

### ksqlDB

**ksqlDB** est un moteur de stream processing qui utilise SQL pour traiter les flux de donn√©es Kafka en temps r√©el.

**Concepts fondamentaux:**

**STREAM (Flux immuable):**
- Repr√©sente un flux de donn√©es en continu
- Append-only (ajout uniquement)
- Chaque √©v√©nement est distinct
- Utilise un topic Kafka standard

**TABLE (√âtat actuel):**
- Repr√©sente l'√©tat actuel d'une entit√©
- Chaque cl√© a une seule valeur (derni√®re valeur)
- Topic compact√© automatiquement
- Mises √† jour par cl√©

**Comparaison:**

| Aspect | STREAM | TABLE |
|--------|--------|-------|
| Nature | Flux d'√©v√©nements | √âtat actuel |
| Donn√©es | Append-only | Mise √† jour par cl√© |
| Topic | Retention standard | Compact√© |
| Exemple | Transactions bancaires | Solde du compte |

### Topics Compact√©s

Les **topics compact√©s** (cleanup.policy=compact) conservent uniquement la derni√®re valeur pour chaque cl√©.

**Mode normal (retention):**
```
Message 1, Message 2, Message 3, Message 4, ...
Apr√®s X temps: tous supprim√©s
```

**Mode compact√©:**
```
Key1: Val1, Key1: Val2, Key2: Val3, Key1: Val4
Apr√®s compaction: Key1: Val4, Key2: Val3
```

**Cas d'usage:**
- Tables ksqlDB (√©tats, agr√©gations)
- Change Data Capture (CDC)
- Caches distribu√©s
- Configuration partag√©e

### Windowing (Fen√™tres Temporelles)

Les **fen√™tres** permettent de grouper les √©v√©nements par intervalle de temps.

**TUMBLING (Fen√™tres fixes):**
```
[0-30s] [30-60s] [60-90s]
```
Fen√™tres qui ne se chevauchent pas.

**HOPPING (Fen√™tres chevauchantes):**
```
[0-60s]
    [30-90s]
        [60-120s]
```
Fen√™tres qui avancent plus rapidement que leur taille.

**SESSION (Fen√™tres d'activit√©):**
```
[Event1...gap...Event2] [Event3...gap...Event4]
```
Fen√™tres bas√©es sur l'inactivit√© (gap entre √©v√©nements).

**Cas d'usage:**
- Analytics en temps r√©el (ventes par heure)
- D√©tection d'anomalies (seuils temporels)
- Agr√©gations glissantes (moyenne mobile)

---

## üéì Progression P√©dagogique

### Matin - Kafka Connect (3h)

1. **Th√©orie** (30 min)
   - Architecture Kafka Connect
   - Types de connecteurs
   - SMTs

2. **Exercice 1** (2h30)
   - D√©ploiement du cluster
   - Source Connector
   - Sink Connector
   - Transformations SMTs
   - Debugging

### Apr√®s-midi - ksqlDB (3h)

1. **Th√©orie** (30 min)
   - STREAM vs TABLE
   - Topics compact√©s
   - Windowing

2. **Exercice 2** (2h30)
   - Cr√©ation de streams
   - Filtres et transformations
   - Tables et agr√©gations
   - Fen√™tres temporelles
   - Joins

---

## üéØ Livrables du Jour 2

√Ä la fin de ce jour, les participants doivent √™tre capables de:

### Kafka Connect
- [ ] D√©ployer un cluster Kafka Connect
- [ ] Cr√©er et configurer des connecteurs JDBC
- [ ] Appliquer des SMTs pour transformer les donn√©es
- [ ] Monitorer les connecteurs via Kafka UI
- [ ] D√©bugger les probl√®mes de connecteurs

### ksqlDB
- [ ] Diff√©rencier STREAM et TABLE
- [ ] Cr√©er des streams avec filtres et transformations
- [ ] Cr√©er des tables avec agr√©gations
- [ ] Impl√©menter des fen√™tres temporelles
- [ ] Joindre streams et tables
- [ ] Comprendre les topics compact√©s

### Concepts Transverses
- [ ] Comprendre l'int√©gration de donn√©es avec Kafka
- [ ] Appliquer le stream processing en temps r√©el
- [ ] Utiliser SQL pour traiter des flux de donn√©es
- [ ] Monitorer et d√©bugger les pipelines de donn√©es

---

## üìö Ressources Compl√©mentaires

### Documentation Officielle
- [Kafka Connect Documentation](https://kafka.apache.org/documentation/#connect)
- [ksqlDB Documentation](https://docs.ksqldb.io/)
- [Confluent Connectors](https://docs.confluent.io/kafka-connectors/self-managed/kafka_connectors.html)

### Guides et Tutoriels
- [Kafka Connect Deep Dive](https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/)
- [ksqlDB Tutorials](https://kafka-tutorials.confluent.io/)
- [Single Message Transforms Guide](https://docs.confluent.io/platform/current/connect/transforms/overview.html)

### Outils
- [Kafka UI](https://github.com/provectus/kafka-ui) - Interface web
- [Confluent Hub](https://www.confluent.io/hub/) - Repository de connecteurs
- [ksqlDB CLI](https://docs.ksqldb.io/en/latest/operate-and-deploy/installation/installing/)

---

## üîß Troubleshooting Commun

### Kafka Connect

**Le connecteur ne d√©marre pas:**
```bash
# Voir les logs
docker logs kafka-connect

# V√©rifier la configuration
curl http://localhost:8083/connectors/<name>/config | jq
```

**Erreurs de conversion:**
- V√©rifiez les converters (JsonConverter vs StringConverter)
- V√©rifiez `schemas.enable`

### ksqlDB

**Le stream ne re√ßoit pas de donn√©es:**
```sql
-- V√©rifier le topic source
PRINT 'topic_name' FROM BEGINNING;

-- V√©rifier les requ√™tes en cours
SHOW QUERIES;
```

**Erreur de format:**
- V√©rifiez `VALUE_FORMAT` (JSON, AVRO, etc.)
- V√©rifiez que le sch√©ma correspond aux donn√©es

---

## üöÄ Pour Aller Plus Loin

### Kafka Connect
- Change Data Capture (CDC) avec Debezium
- Connecteurs pour S3, Elasticsearch, MongoDB
- Mode distribu√© en production
- Custom SMTs en Java

### ksqlDB
- User-Defined Functions (UDFs)
- Pull Queries (requ√™tes ponctuelles)
- Stream-Stream Joins
- Schema Registry avec Avro
- ksqlDB Connectors

---

**Pr√™t √† d√©marrer? Commencez par l'[Exercice 1 - Kafka Connect](./exercice1-kafka-connect/)! üöÄ**
