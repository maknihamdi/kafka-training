# Exercice All-in-One - Plateforme de Devis d'Assurance

## üéØ Objectifs P√©dagogiques

### Architecture Compl√®te Kafka
- Pipeline compl√®te de traitement temps r√©el avec Kafka Streams
- Pattern CQRS (Command Query Responsibility Segregation)
- Int√©gration Kafka Connect pour synchronisation vers Redis
- API REST de lecture et d'√©criture
- Monitoring complet avec Prometheus et Grafana

### Kafka Streams
- Manipulation de KStream et KTable
- Filtrage et transformations de flux
- Jointures stream-table pour enrichissement
- Agr√©gations avec fen√™tres temporelles
- Topics compact√©s pour donn√©es de r√©f√©rence

### Int√©gration & Architecture
- Kafka Connect avec Redis Sink Connector
- Redis comme cache de lecture (read model)
- S√©paration write model (Producer) / read model (Query API)
- Module commun pour partage de mod√®les

### Monitoring
- 482 m√©triques Kafka expos√©es via Prometheus
- 3 dashboards Grafana pr√©-configur√©s (26 panels)
- M√©triques Producer, Streams et Broker en temps r√©el

---

## üìä Architecture Globale

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Producer API   ‚îÇ (Port 8081)
‚îÇ  Write Model    ‚îÇ POST /api/quotes
‚îÇ                 ‚îÇ POST /api/pricing/init
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Kafka Topics               ‚îÇ
‚îÇ  ‚Ä¢ devis-events                 ‚îÇ
‚îÇ  ‚Ä¢ product-pricing (compact√©)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Kafka Streams   ‚îÇ (Port 8082)
‚îÇ  Topologie:     ‚îÇ
‚îÇ  - Filter       ‚îÇ ‚Üê Devis valid√©s uniquement
‚îÇ  - Join         ‚îÇ ‚Üê Enrichissement avec prix
‚îÇ  - Transform    ‚îÇ ‚Üê Calcul prime finale
‚îÇ  - Aggregate    ‚îÇ ‚Üê Stats par client
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Output Topics               ‚îÇ
‚îÇ  ‚Ä¢ validated-quotes             ‚îÇ
‚îÇ  ‚Ä¢ all-quotes (‚Üí Redis)         ‚îÇ
‚îÇ  ‚Ä¢ quote-aggregates             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Kafka Connect   ‚îÇ (Port 8083)
‚îÇ  Redis Sink     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Redis       ‚îÇ ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ  Prometheus  ‚îÇ
‚îÇ   (Cache)       ‚îÇ        ‚îÇ   Grafana    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Query API     ‚îÇ (Port 8084)
‚îÇ   Read Model    ‚îÇ GET /api/quotes
‚îÇ                 ‚îÇ GET /api/quotes/stats
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìã Pr√©requis

- Docker et Docker Compose
- Java 17+
- Maven 3.8+
- Ports disponibles : 8080-8085, 9090, 3000, 9092, 6379

---

## ‚ö†Ô∏è NOTE IMPORTANTE : Ordre de D√©marrage

**L'ordre des √©tapes est CRUCIAL pour √©viter les erreurs !**

Kafka Streams a besoin que les topics d'entr√©e existent **AVANT** de d√©marrer. Suivez cet ordre :

1. ‚úÖ D√©marrer l'infrastructure Docker (Kafka, Redis, etc.)
2. ‚úÖ Compiler les projets Maven
3. ‚úÖ Lancer le **Producer**
4. ‚úÖ **Initialiser les donn√©es de r√©f√©rence** (create topics)
5. ‚úÖ Lancer **Kafka Streams** (apr√®s cr√©ation des topics)
6. ‚úÖ Configurer Kafka Connect
7. ‚úÖ Lancer le Query API

---

## üöÄ PARTIE 1 - D√©marrage de l'Infrastructure

### √âtape 1.1 : Lancer tous les services Docker

```bash
make start
```

Cette commande d√©marre :
- ‚úÖ **Kafka** (port 9092) - Broker en mode KRaft
- ‚úÖ **Kafka UI** (port 8080) - Interface web
- ‚úÖ **Redis** (port 6379) - Cache pour read model
- ‚úÖ **Redis Commander** (port 8085) - UI Redis
- ‚úÖ **Kafka Connect** (port 8083) - Connecteur Redis
- ‚úÖ **Prometheus** (port 9090) - Collecte de m√©triques
- ‚úÖ **Grafana** (port 3000) - Visualisation m√©triques

‚è≥ **Attendre 40 secondes** pour le d√©marrage complet.

### √âtape 1.2 : V√©rifier les services

```bash
make status
```

Tous les services doivent √™tre **UP** :
```
kafka            Up
kafka-ui         Up
redis            Up
redis-commander  Up
kafka-connect    Up
prometheus       Up
grafana          Up
```

### √âtape 1.3 : Compiler les projets Maven

```bash
make build
```

Cela compile les 4 modules :
- ‚úÖ **common** - Mod√®les partag√©s
- ‚úÖ **producer** - API d'√©criture
- ‚úÖ **streams** - Traitement temps r√©el
- ‚úÖ **query-api** - API de lecture

---

## üì¶ PARTIE 2 - Structure du Projet

```
exercice1-all-in-one/
‚îú‚îÄ‚îÄ common/                    # Module partag√©
‚îÇ   ‚îî‚îÄ‚îÄ src/main/java/.../model/
‚îÇ       ‚îú‚îÄ‚îÄ Quote.java         # Devis d'assurance
‚îÇ       ‚îú‚îÄ‚îÄ QuoteStatus.java   # DRAFT/VALIDATED/CANCELLED/EXPIRED
‚îÇ       ‚îú‚îÄ‚îÄ ProductPricing.java    # R√©f√©rentiel de prix
‚îÇ       ‚îú‚îÄ‚îÄ EnrichedQuote.java     # Devis enrichi
‚îÇ       ‚îî‚îÄ‚îÄ QuoteAggregate.java    # Statistiques

‚îú‚îÄ‚îÄ producer/                  # API d'√©criture (Port 8081)
‚îÇ   ‚îî‚îÄ‚îÄ controller/QuoteController.java
‚îÇ       ‚îú‚îÄ‚îÄ POST /api/quotes           # Cr√©er un devis
‚îÇ       ‚îú‚îÄ‚îÄ POST /api/quotes/{id}/validate
‚îÇ       ‚îú‚îÄ‚îÄ POST /api/quotes/{id}/cancel
‚îÇ       ‚îú‚îÄ‚îÄ POST /api/quotes/generate  # G√©n√©rer N devis
‚îÇ       ‚îî‚îÄ‚îÄ POST /api/pricing/init     # Init r√©f√©rentiel

‚îú‚îÄ‚îÄ streams/                   # Traitement (Port 8082)
‚îÇ   ‚îî‚îÄ‚îÄ topology/QuoteStreamTopology.java
‚îÇ       ‚îú‚îÄ‚îÄ Filter: devis valid√©s
‚îÇ       ‚îú‚îÄ‚îÄ Join: enrichissement avec prix
‚îÇ       ‚îú‚îÄ‚îÄ Transform: calcul prime finale
‚îÇ       ‚îî‚îÄ‚îÄ Aggregate: stats par client

‚îú‚îÄ‚îÄ query-api/                 # API de lecture (Port 8084)
‚îÇ   ‚îî‚îÄ‚îÄ controller/QuoteQueryController.java
‚îÇ       ‚îú‚îÄ‚îÄ GET /api/quotes            # Tous les devis
‚îÇ       ‚îú‚îÄ‚îÄ GET /api/quotes/{id}
‚îÇ       ‚îú‚îÄ‚îÄ GET /api/quotes/customer/{id}
‚îÇ       ‚îú‚îÄ‚îÄ GET /api/quotes/product/{code}
‚îÇ       ‚îî‚îÄ‚îÄ GET /api/quotes/stats

‚îî‚îÄ‚îÄ docker-compose.yml         # Infrastructure compl√®te
```

---

## üîß PARTIE 3 - D√©marrage des Applications

### √âtape 3.1 : Lancer le Producer (Terminal 1)

```bash
make run-producer
```

Le Producer d√©marre sur **http://localhost:8081** et :
- ‚úÖ Cr√©e automatiquement les topics Kafka via l'API Admin
- ‚úÖ Expose l'API REST pour cr√©er des devis
- ‚úÖ Expose les m√©triques Prometheus sur `/actuator/prometheus`

**Topics cr√©√©s automatiquement :**
- `devis-events` - Tous les √©v√©nements de devis
- `product-pricing` - R√©f√©rentiel de prix (compact√©)

**V√©rifier que le Producer fonctionne :**
```bash
curl http://localhost:8081/actuator/health
# R√©ponse attendue: {"status":"UP"}
```

### √âtape 3.2 : Initialiser les Donn√©es de R√©f√©rence

‚ö†Ô∏è **IMPORTANT :** Il faut cr√©er les topics d'entr√©e AVANT de lancer Kafka Streams !

```bash
make init-pricing
```

**Ce qui se passe :**
1. ‚úÖ 5 produits sont cr√©√©s dans `product-pricing` (topic compact√©)
2. ‚úÖ Le topic `product-pricing` est automatiquement cr√©√©
3. ‚úÖ Kafka Streams pourra ensuite charger ces donn√©es dans un KTable

**Produits cr√©√©s :**
| Code | Nom | Prix de base | Taxe |
|------|-----|--------------|------|
| AUTO | Auto Insurance | 500‚Ç¨ | 20% |
| HOME | Home Insurance | 800‚Ç¨ | 15% |
| HEALTH | Health Insurance | 1200‚Ç¨ | 10% |
| LIFE | Life Insurance | 2000‚Ç¨ | 5% |
| TRAVEL | Travel Insurance | 150‚Ç¨ | 25% |

**V√©rifier dans Kafka UI (http://localhost:8080) :**
- Topic `product-pricing` ‚Üí 5 messages
- Observer `cleanup.policy=compact`

### √âtape 3.3 : Lancer Kafka Streams (Terminal 2)

```bash
make run-streams
```

Kafka Streams d√©marre sur **http://localhost:8082** et :
- ‚úÖ Lit les topics d'entr√©e (`devis-events` et `product-pricing`)
- ‚úÖ Cr√©e les topics de sortie automatiquement
- ‚úÖ D√©marre la topologie de traitement
- ‚úÖ Expose les m√©triques Prometheus

**Topics cr√©√©s par Streams :**
- `validated-quotes` - Devis valid√©s uniquement
- `all-quotes` - Devis enrichis (‚Üí Redis)
- `quote-aggregates` - Statistiques par client

**V√©rifier les logs :** La topologie s'affiche au d√©marrage sans erreur.

### √âtape 3.4 : Configurer Kafka Connect vers Redis

```bash
make connect-redis
```

Cette commande configure le **Redis Sink Connector** qui :
- ‚úÖ Lit le topic `all-quotes`
- ‚úÖ √âcrit chaque devis enrichi dans Redis
- ‚úÖ Utilise quoteId comme cl√© Redis

**V√©rifier le connecteur :**
```bash
make connect-status
# Doit afficher: ["redis-sink-quotes"]
```

### √âtape 3.5 : Lancer le Query API (Terminal 3)

```bash
make run-query-api
```

Query API d√©marre sur **http://localhost:8084** et :
- ‚úÖ Se connecte √† Redis
- ‚úÖ Expose l'API REST de lecture
- ‚úÖ Expose les m√©triques Prometheus

**V√©rifier le Query API :**
```bash
curl http://localhost:8084/api/quotes/health
# R√©ponse: {"status":"UP","service":"query-api"}
```

---

## üìä PARTIE 4 - Alimentation de la Pipeline (2 Vagues Suppl√©mentaires)

‚ö†Ô∏è **Note :** La VAGUE 1 (initialisation du r√©f√©rentiel) a d√©j√† √©t√© effectu√©e √† l'√©tape 3.2.

### üåä VAGUE 2 : Cr√©ation de Devis (Mix d'√âtats)

**Objectif :** G√©n√©rer 20 devis avec diff√©rents statuts pour tester le filtrage.

```bash
make generate-quotes
```

**Ce qui se passe :**
1. ‚úÖ 20 devis cr√©√©s avec statuts al√©atoires :
   - ~12 devis **VALIDATED** (60%)
   - ~4 devis **DRAFT** (20%)
   - ~4 devis **CANCELLED** (20%)

2. ‚úÖ **Producer** ‚Üí √©crit dans `devis-events`

3. ‚úÖ **Kafka Streams** :
   - Filtre ‚Üí Garde uniquement les VALIDATED (~12)
   - Join ‚Üí Enrichit avec `product-pricing` (KTable)
   - Transform ‚Üí Calcule `finalPremium = basePremium * (1 + taxRate)`
   - √âcrit dans `all-quotes`

4. ‚úÖ **Kafka Connect** ‚Üí Synchronise vers Redis

5. ‚úÖ **Query API** ‚Üí Peut maintenant lire depuis Redis

**Attendre 5 secondes** pour la propagation compl√®te.

**V√©rifier le filtrage :**
```bash
# Topic d'entr√©e : tous les devis (20)
make consume-quotes

# Topic filtr√© : devis valid√©s uniquement (~12)
make consume-validated
```

**V√©rifier l'enrichissement :**
```bash
# Devis enrichis avec productName, basePrice, taxRate, finalPremium
make consume-all-quotes
```

Exemple de devis enrichi :
```json
{
  "quoteId": "Q-a3f2b1c4",
  "customerId": "C002",
  "status": "VALIDATED",
  "productCode": "AUTO",
  "basePremium": 450.0,
  "finalPremium": 540.0,      ‚Üê Calcul√©: 450 * 1.20
  "productName": "Auto Insurance",
  "basePrice": 500.0,
  "taxRate": 0.20,
  "createdAt": 1702394821000,
  "updatedAt": 1702394821000
}
```

**V√©rifier dans Redis :**
```bash
make redis-cli
> KEYS Q-*
# Devrait afficher ~12 cl√©s (devis valid√©s)
> GET Q-a3f2b1c4
# Affiche le JSON complet du devis enrichi
> exit
```

**V√©rifier via Query API :**
```bash
# Tous les devis
make query-all

# Statistiques globales
make query-stats
```

R√©ponse attendue de `/stats` :
```json
{
  "totalQuotes": 12,
  "totalPremium": 8450.50,
  "averagePremium": 704.21,
  "byProduct": {
    "AUTO": 3,
    "HOME": 2,
    "HEALTH": 4,
    "LIFE": 1,
    "TRAVEL": 2
  }
}
```

---

### üåä VAGUE 3 : √âvolution des Devis (Changements d'√âtat)

**Objectif :** Simuler le cycle de vie des devis avec validations et annulations.

#### 3.1 G√©n√©rer 50 nouveaux devis

```bash
curl -X POST "http://localhost:8081/api/quotes/generate?count=50"
```

**R√©sultat :** ~30 valid√©s, ~10 draft, ~10 cancelled.

**Attendre 5 secondes** puis v√©rifier Redis :
```bash
make redis-cli
> DBSIZE
# Devrait afficher ~42 (12 + 30 nouveaux valid√©s)
> exit
```

#### 3.2 Valider un devis sp√©cifique

```bash
curl -X POST "http://localhost:8081/api/quotes/Q-12345678/validate"
```

**Ce qui se passe :**
1. ‚úÖ Producer cr√©e un √©v√©nement avec `status=VALIDATED`
2. ‚úÖ Streams filtre et enrichit
3. ‚úÖ Kafka Connect √©crit dans Redis
4. ‚úÖ Query API peut le lire imm√©diatement

**V√©rifier :**
```bash
curl http://localhost:8084/api/quotes/Q-12345678
```

#### 3.3 Annuler un devis

```bash
curl -X POST "http://localhost:8081/api/quotes/Q-12345678/cancel"
```

**Ce qui se passe :**
1. ‚úÖ Producer cr√©e un √©v√©nement avec `status=CANCELLED`
2. ‚úÖ Streams **ignore** car pas VALIDATED
3. ‚úÖ Le devis **reste dans Redis** (ancienne version valid√©e)

‚ö†Ô∏è **Note :** Dans cette version simplifi√©e, les annulations ne suppriment pas de Redis. Pour une vraie gestion, il faudrait :
- Soit √©crire `null` dans `all-quotes` (Kafka tombstone)
- Soit un processus de nettoyage dans Query API

#### 3.4 V√©rifier les agr√©gations par client

```bash
# Consommer le topic d'agr√©gations
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic quote-aggregates \
  --from-beginning \
  --max-messages 5
```

**R√©sultat attendu :** Statistiques par client avec fen√™tre de 1h :
```json
{
  "customerId": "C001",
  "windowStart": 1702394400000,
  "windowEnd": 1702398000000,
  "count": 8,
  "totalPremium": 4850.75
}
```

#### 3.5 Requ√™tes Query API par client

```bash
# Tous les devis du client C001
make query-customer C=C001

# Tous les devis Auto
make query-product P=AUTO
```

---

## üìä PARTIE 5 - Visualisation et Monitoring

### √âtape 5.1 : Kafka UI

Ouvrir **http://localhost:8080**

**Exploration des topics :**
1. Topics ‚Üí `devis-events` ‚Üí Messages
   - Observer les diff√©rents statuts
2. Topics ‚Üí `product-pricing` ‚Üí Messages
   - Observer `cleanup.policy=compact`
   - Voir les cl√©s (productCode)
3. Topics ‚Üí `all-quotes` ‚Üí Messages
   - Uniquement des devis enrichis

**Consumer Groups :**
- `event-processor` ‚Üí Application Kafka Streams
- Observer le lag (devrait √™tre proche de 0)

### √âtape 5.2 : Redis Commander

Ouvrir **http://localhost:8085**

**Exploration :**
- Voir toutes les cl√©s `Q-*`
- Cliquer sur une cl√© pour voir le JSON complet
- Observer la structure EnrichedQuote

### √âtape 5.3 : Prometheus

Ouvrir **http://localhost:9090**

**Requ√™tes utiles :**

```promql
# Taux d'envoi Producer
rate(kafka_producer_record_send_total[1m])

# Taux de traitement Streams
rate(kafka_stream_task_process_total[1m])

# Consumer lag
sum(kafka_consumer_fetch_manager_records_lag) by (topic)

# Throughput Broker (bytes/sec)
rate(kafka_server_brokertopicmetrics_bytesin_total[1m])
```

### √âtape 5.4 : Grafana

Ouvrir **http://localhost:3000** (admin/admin)

**Dashboards disponibles :**

1. **Kafka Producer Metrics** (6 panels)
   - Records Sent Rate
   - Request Latency
   - Buffer Memory
   - Batch Size
   - Error & Retry Rate
   - Waiting Threads

2. **Kafka Streams Metrics** (9 panels)
   - Process Rate
   - Commit Latency
   - Consumer Lag par partition
   - Poll Records Average
   - Alive Threads
   - Assigned Partitions
   - State Store Size
   - Poll Latency
   - Punctuate Latency

3. **Kafka Broker Metrics** (11 panels)
   - Network Throughput
   - Messages In Rate
   - Request Latency
   - Under Replicated Partitions
   - Log Size par partition
   - CPU Usage
   - JVM Heap Memory
   - GC Collection Time
   - Thread Count

---

## üéØ PARTIE 6 - Tests de Bout en Bout

### Test 1 : Pipeline Compl√®te

```bash
# 1. Cr√©er un devis
QUOTE_ID=$(curl -s -X POST http://localhost:8081/api/quotes \
  -H "Content-Type: application/json" \
  -d '{
    "customerId": "C001",
    "status": "DRAFT",
    "productCode": "AUTO",
    "basePremium": 450.0
  }' | jq -r '.quoteId')

# 2. Le devis n'est pas encore dans Redis (DRAFT)
curl http://localhost:8084/api/quotes/$QUOTE_ID
# ‚Üí 404 Not Found

# 3. Valider le devis
curl -X POST "http://localhost:8081/api/quotes/$QUOTE_ID/validate"

# 4. Attendre 2 secondes
sleep 2

# 5. Le devis est maintenant dans Redis (enrichi)
curl http://localhost:8084/api/quotes/$QUOTE_ID
# ‚Üí 200 OK avec finalPremium calcul√©e
```

### Test 2 : G√©n√©ration Massive

```bash
# G√©n√©rer 100 devis
make generate-100

# Attendre 10 secondes
sleep 10

# V√©rifier les statistiques
make query-stats
```

### Test 3 : Visualiser la Topologie

```bash
make topology-describe
```

Copier la sortie et coller sur : https://zz85.github.io/kafka-streams-viz/

**Vous verrez graphiquement :**
- Source: devis-events
- Source: product-pricing (table)
- Filter: status == VALIDATED
- Join: quotes + pricing
- Sink: all-quotes
- Aggregate: fen√™tre 1h

---

## üéì PARTIE 7 - Concepts Cl√©s D√©montr√©s

### 7.1 Pattern CQRS

| Write Model | Read Model |
|-------------|------------|
| Producer API (8081) | Query API (8084) |
| POST /api/quotes | GET /api/quotes |
| √âcrit dans Kafka | Lit depuis Redis |
| Optimis√© pour √©criture | Optimis√© pour lecture |

### 7.2 Kafka Streams Operations

```java
// 1. Filter
quotesStream.filter((k, v) -> v.getStatus() == VALIDATED)

// 2. Join (Stream + Table)
quotesStream.join(pricingTable, (quote, pricing) -> {...})

// 3. Transform
quote.setFinalPremium(basePremium * (1 + taxRate))

// 4. Aggregate (avec fen√™tre)
.windowedBy(TimeWindows.of(Duration.ofHours(1)))
.aggregate(...)
```

### 7.3 Topic Compact√© vs Normal

| Aspect | Normal (devis-events) | Compact√© (product-pricing) |
|--------|----------------------|----------------------------|
| Cleanup | delete (par temps) | compact (par cl√©) |
| Historique | Tous les √©v√©nements | Derni√®re valeur uniquement |
| Usage | KStream | KTable |
| Exemple | Flux de devis | R√©f√©rentiel de prix |

### 7.4 Kafka Connect

- **Source Connector** : Externe ‚Üí Kafka
- **Sink Connector** : Kafka ‚Üí Externe (Redis dans notre cas)
- **Avantage** : Pas de code, juste configuration JSON

---

## üß™ PARTIE 8 - Exp√©rimentations Avanc√©es

### Exp√©rience 1 : Mettre √† Jour un Prix

```bash
curl -X POST http://localhost:8081/api/pricing \
  -H "Content-Type: application/json" \
  -d '{
    "productCode": "AUTO",
    "productName": "Auto Insurance PROMO",
    "basePrice": 400.0,
    "taxRate": 0.18
  }'
```

**R√©sultat :**
- ‚úÖ Le topic compact√© garde la nouvelle valeur
- ‚úÖ Le KTable se met √† jour automatiquement
- ‚úÖ Les prochains devis AUTO auront le nouveau prix

### Exp√©rience 2 : Stress Test

```bash
# G√©n√©rer 500 devis rapidement
for i in {1..5}; do
  curl -X POST "http://localhost:8081/api/quotes/generate?count=100" &
done
wait
```

**Observer dans Grafana :**
- Process Rate augmente
- Consumer Lag monte puis redescend
- JVM Heap Memory utilis√©

### Exp√©rience 3 : Red√©marrer Streams

```bash
# 1. Arr√™ter Kafka Streams (Ctrl+C dans Terminal 2)
# 2. G√©n√©rer des devis
make generate-quotes
# 3. Relancer Kafka Streams
make run-streams
```

**R√©sultat :**
- ‚úÖ Streams rattrape le lag automatiquement
- ‚úÖ Tous les devis sont trait√©s
- ‚úÖ Rien n'est perdu (durabilit√© Kafka)

---

## üìö PARTIE 9 - Documentation Compl√®te

### Commandes Make Disponibles

```bash
# Infrastructure
make start           # D√©marrer tout
make stop            # Arr√™ter tout
make clean           # Supprimer volumes
make status          # V√©rifier statut

# Build & Run
make build           # Compiler Maven
make run-producer    # Lancer Producer
make run-streams     # Lancer Streams
make run-query-api   # Lancer Query API

# Donn√©es
make init-pricing    # Init r√©f√©rentiel
make generate-quotes # 20 devis
make generate-100    # 100 devis

# Topics Kafka
make topics          # Lister topics
make describe-topics # D√©tails topics
make consume-quotes  # Consommer devis
make consume-validated # Consommer valid√©s
make consume-all-quotes # Consommer enrichis

# Query API
make query-all       # GET /quotes
make query-stats     # GET /quotes/stats
make query-customer C=C001 # Par client
make query-product P=AUTO  # Par produit

# Redis
make redis-cli       # Redis CLI
make redis-ui        # Ouvrir Commander

# Kafka Connect
make connect-status  # Statut
make connect-redis   # Configurer

# Monitoring
make prometheus      # Ouvrir Prometheus
make grafana         # Ouvrir Grafana
make topology        # Topologie JSON
make topology-describe # Topologie texte
```

### URLs des Services

| Service | URL | Credentials |
|---------|-----|-------------|
| Producer API | http://localhost:8081 | - |
| Streams API | http://localhost:8082 | - |
| Kafka Connect | http://localhost:8083 | - |
| Query API | http://localhost:8084 | - |
| Kafka UI | http://localhost:8080 | - |
| Redis Commander | http://localhost:8085 | - |
| Prometheus | http://localhost:9090 | - |
| Grafana | http://localhost:3000 | admin/admin |

### Fichiers de Documentation

- `QUICKSTART.md` - D√©marrage rapide
- `MONITORING.md` - Guide monitoring complet
- `METRICS_REFERENCE.md` - R√©f√©rence 482 m√©triques
- `REFACTORING_SUMMARY.md` - Architecture d√©taill√©e
- `PLAN_REFACTORING_ASSURANCE.md` - Plan de refactoring

---

## üßπ Nettoyage

```bash
# Arr√™ter proprement
Ctrl+C dans chaque terminal (Producer, Streams, Query API)

# Arr√™ter Docker
make stop

# Nettoyage complet (supprime volumes)
make clean
```

---

## üéØ Points Cl√©s de l'Exercice

‚úÖ **Aucun d√©veloppement requis** - Tout est pr√©-impl√©ment√©
‚úÖ **3 vagues d'alimentation** - Pour voir l'√©volution des donn√©es
‚úÖ **Pipeline compl√®te** - Du Producer au Query API via Redis
‚úÖ **Monitoring temps r√©el** - Prometheus + Grafana
‚úÖ **Pattern CQRS** - S√©paration write/read
‚úÖ **Kafka Streams** - Filter, Join, Transform, Aggregate
‚úÖ **Architecture modulaire** - 4 modules Maven

---

## üéâ F√©licitations !

Vous avez maintenant une **plateforme compl√®te de traitement de devis d'assurance** avec :
- Kafka Streams pour le traitement temps r√©el
- Redis pour le cache de lecture
- Kafka Connect pour la synchronisation
- Monitoring complet avec Prometheus et Grafana
- APIs REST s√©par√©es (write/read)

**Cette architecture est utilis√©e en production dans de nombreuses entreprises !** üöÄ
