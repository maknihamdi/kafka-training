# Exercice All-in-One - Plateforme de Devis d'Assurance

## ðŸŽ¯ Objectifs PÃ©dagogiques

### Architecture ComplÃ¨te Kafka
- Pipeline complÃ¨te de traitement temps rÃ©el avec Kafka Streams
- Pattern CQRS (Command Query Responsibility Segregation)
- IntÃ©gration Kafka Connect pour synchronisation vers Redis
- API REST de lecture et d'Ã©criture
- Monitoring complet avec Prometheus et Grafana

### Kafka Streams
- Manipulation de KStream et KTable
- Filtrage et transformations de flux
- Jointures stream-table pour enrichissement
- AgrÃ©gations avec fenÃªtres temporelles
- Topics compactÃ©s pour donnÃ©es de rÃ©fÃ©rence

### IntÃ©gration & Architecture
- Kafka Connect avec Redis Sink Connector
- Redis comme cache de lecture (read model)
- SÃ©paration write model (Producer) / read model (Query API)
- Module commun pour partage de modÃ¨les

### Monitoring
- 482 mÃ©triques Kafka exposÃ©es via Prometheus
- 3 dashboards Grafana prÃ©-configurÃ©s (26 panels)
- MÃ©triques Producer, Streams et Broker en temps rÃ©el

---

## ðŸ“Š Architecture Globale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Producer API   â”‚ (Port 8081)
â”‚  Write Model    â”‚ POST /api/quotes
â”‚                 â”‚ POST /api/pricing/init
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Kafka Topics               â”‚
â”‚  â€¢ devis-events                 â”‚
â”‚  â€¢ product-pricing (compactÃ©)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Streams   â”‚ (Port 8082)
â”‚  Topologie:     â”‚
â”‚  - Filter       â”‚ â† Devis validÃ©s uniquement
â”‚  - Join         â”‚ â† Enrichissement avec prix
â”‚  - Transform    â”‚ â† Calcul prime finale
â”‚  - Aggregate    â”‚ â† Stats par client
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Output Topics               â”‚
â”‚  â€¢ validated-quotes             â”‚
â”‚  â€¢ all-quotes (â†’ Redis)         â”‚
â”‚  â€¢ quote-aggregates             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Connect   â”‚ (Port 8083)
â”‚  Redis Sink     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Redis       â”‚ â†â”€â”€â”€â”€  â”‚  Prometheus  â”‚
â”‚   (Cache)       â”‚        â”‚   Grafana    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Query API     â”‚ (Port 8084)
â”‚   Read Model    â”‚ GET /api/quotes
â”‚                 â”‚ GET /api/quotes/stats
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“‹ PrÃ©requis

- Docker et Docker Compose
- Java 17+
- Maven 3.8+
- Ports disponibles : 8080-8085, 9090, 3000, 9092, 6379

---

## âš ï¸ NOTE IMPORTANTE : Ordre de DÃ©marrage

**L'ordre des Ã©tapes est CRUCIAL pour Ã©viter les erreurs !**

Kafka Streams a besoin que les topics d'entrÃ©e existent **AVANT** de dÃ©marrer. Suivez cet ordre :

1. âœ… DÃ©marrer l'infrastructure Docker (Kafka, Redis, etc.)
2. âœ… Compiler les projets Maven
3. âœ… Lancer le **Producer**
4. âœ… **Initialiser les donnÃ©es de rÃ©fÃ©rence** (create topics)
5. âœ… Lancer **Kafka Streams** (aprÃ¨s crÃ©ation des topics)
6. âœ… Configurer Kafka Connect
7. âœ… Lancer le Query API

---

## ðŸš€ PARTIE 1 - DÃ©marrage de l'Infrastructure

### Ã‰tape 1.1 : Lancer tous les services Docker

```bash
make start
```

Cette commande dÃ©marre :
- âœ… **Kafka** (port 9092) - Broker en mode KRaft
- âœ… **Kafka UI** (port 8080) - Interface web
- âœ… **Redis** (port 6379) - Cache pour read model
- âœ… **Redis Commander** (port 8085) - UI Redis
- âœ… **Kafka Connect** (port 8083) - Connecteur Redis
- âœ… **Prometheus** (port 9090) - Collecte de mÃ©triques
- âœ… **Grafana** (port 3000) - Visualisation mÃ©triques

â³ **Attendre 40 secondes** pour le dÃ©marrage complet.

### Ã‰tape 1.2 : VÃ©rifier les services

```bash
make status
```

Tous les services doivent Ãªtre **UP** :
```
kafka            Up
kafka-ui         Up
redis            Up
redis-commander  Up
kafka-connect    Up
prometheus       Up
grafana          Up
```

### Ã‰tape 1.3 : Compiler les projets Maven

```bash
make build
```

Cela compile les 4 modules :
- âœ… **common** - ModÃ¨les partagÃ©s
- âœ… **producer** - API d'Ã©criture
- âœ… **streams** - Traitement temps rÃ©el
- âœ… **query-api** - API de lecture

---

## ðŸ“¦ PARTIE 2 - Structure du Projet

```
exercice1-all-in-one/
â”œâ”€â”€ common/                    # Module partagÃ©
â”‚   â””â”€â”€ src/main/java/.../model/
â”‚       â”œâ”€â”€ Quote.java         # Devis d'assurance
â”‚       â”œâ”€â”€ QuoteStatus.java   # DRAFT/VALIDATED/CANCELLED/EXPIRED
â”‚       â”œâ”€â”€ ProductPricing.java    # RÃ©fÃ©rentiel de prix
â”‚       â”œâ”€â”€ EnrichedQuote.java     # Devis enrichi
â”‚       â””â”€â”€ QuoteAggregate.java    # Statistiques

â”œâ”€â”€ producer/                  # API d'Ã©criture (Port 8081)
â”‚   â””â”€â”€ controller/QuoteController.java
â”‚       â”œâ”€â”€ POST /api/quotes           # CrÃ©er un devis
â”‚       â”œâ”€â”€ POST /api/quotes/{id}/validate
â”‚       â”œâ”€â”€ POST /api/quotes/{id}/cancel
â”‚       â”œâ”€â”€ POST /api/quotes/generate  # GÃ©nÃ©rer N devis
â”‚       â””â”€â”€ POST /api/pricing/init     # Init rÃ©fÃ©rentiel

â”œâ”€â”€ streams/                   # Traitement (Port 8082)
â”‚   â””â”€â”€ topology/QuoteStreamTopology.java
â”‚       â”œâ”€â”€ Filter: devis validÃ©s
â”‚       â”œâ”€â”€ Join: enrichissement avec prix
â”‚       â”œâ”€â”€ Transform: calcul prime finale
â”‚       â””â”€â”€ Aggregate: stats par client

â”œâ”€â”€ query-api/                 # API de lecture (Port 8084)
â”‚   â””â”€â”€ controller/QuoteQueryController.java
â”‚       â”œâ”€â”€ GET /api/quotes            # Tous les devis
â”‚       â”œâ”€â”€ GET /api/quotes/{id}
â”‚       â”œâ”€â”€ GET /api/quotes/customer/{id}
â”‚       â”œâ”€â”€ GET /api/quotes/product/{code}
â”‚       â””â”€â”€ GET /api/quotes/stats

â””â”€â”€ docker-compose.yml         # Infrastructure complÃ¨te
```

---

## ðŸ”§ PARTIE 3 - DÃ©marrage des Applications

### Ã‰tape 3.1 : Lancer le Producer (Terminal 1)

```bash
make run-producer
```

Le Producer dÃ©marre sur **http://localhost:8081** et :
- âœ… CrÃ©e automatiquement les topics Kafka via l'API Admin
- âœ… Expose l'API REST pour crÃ©er des devis
- âœ… Expose les mÃ©triques Prometheus sur `/actuator/prometheus`

**Topics crÃ©Ã©s automatiquement :**
- `devis-events` - Tous les Ã©vÃ©nements de devis
- `product-pricing` - RÃ©fÃ©rentiel de prix (compactÃ©)

**VÃ©rifier que le Producer fonctionne :**
```bash
curl http://localhost:8081/actuator/health
# RÃ©ponse attendue: {"status":"UP"}
```

### Ã‰tape 3.2 : Initialiser les DonnÃ©es de RÃ©fÃ©rence

âš ï¸ **IMPORTANT :** Il faut crÃ©er les topics d'entrÃ©e AVANT de lancer Kafka Streams !

```bash
make init-pricing
```

**Ce qui se passe :**
1. âœ… 5 produits sont crÃ©Ã©s dans `product-pricing` (topic compactÃ©)
2. âœ… Le topic `product-pricing` est automatiquement crÃ©Ã©
3. âœ… Kafka Streams pourra ensuite charger ces donnÃ©es dans un KTable

**Produits crÃ©Ã©s :**
| Code | Nom | Prix de base | Taxe |
|------|-----|--------------|------|
| AUTO | Auto Insurance | 500â‚¬ | 20% |
| HOME | Home Insurance | 800â‚¬ | 15% |
| HEALTH | Health Insurance | 1200â‚¬ | 10% |
| LIFE | Life Insurance | 2000â‚¬ | 5% |
| TRAVEL | Travel Insurance | 150â‚¬ | 25% |

**VÃ©rifier dans Kafka UI (http://localhost:8080) :**
- Topic `product-pricing` â†’ 5 messages
- Observer `cleanup.policy=compact`

### Ã‰tape 3.3 : Lancer Kafka Streams (Terminal 2)

```bash
make run-streams
```

Kafka Streams dÃ©marre sur **http://localhost:8082** et :
- âœ… Lit les topics d'entrÃ©e (`devis-events` et `product-pricing`)
- âœ… CrÃ©e les topics de sortie automatiquement
- âœ… DÃ©marre la topologie de traitement
- âœ… Expose les mÃ©triques Prometheus

**Topics crÃ©Ã©s par Streams :**
- `validated-quotes` - Devis validÃ©s uniquement
- `all-quotes` - Devis enrichis (â†’ Redis)
- `quote-aggregates` - Statistiques par client

**VÃ©rifier les logs :** La topologie s'affiche au dÃ©marrage sans erreur.

### Ã‰tape 3.4 : Configurer Kafka Connect vers Redis

```bash
make connect-redis
```

Cette commande configure le **Redis Sink Connector** qui :
- âœ… Lit le topic `all-quotes`
- âœ… Ã‰crit chaque devis enrichi dans Redis
- âœ… Utilise quoteId comme clÃ© Redis

**VÃ©rifier le connecteur :**
```bash
make connect-status
# Doit afficher: ["redis-sink-quotes"]
```

### Ã‰tape 3.5 : Lancer le Query API (Terminal 3)

```bash
make run-query-api
```

Query API dÃ©marre sur **http://localhost:8084** et :
- âœ… Se connecte Ã  Redis
- âœ… Expose l'API REST de lecture
- âœ… Expose les mÃ©triques Prometheus

**VÃ©rifier le Query API :**
```bash
curl http://localhost:8084/api/quotes/health
# RÃ©ponse: {"status":"UP","service":"query-api"}
```

---

## ðŸ“Š PARTIE 4 - Alimentation de la Pipeline (2 Vagues SupplÃ©mentaires)

âš ï¸ **Note :** La VAGUE 1 (initialisation du rÃ©fÃ©rentiel) a dÃ©jÃ  Ã©tÃ© effectuÃ©e Ã  l'Ã©tape 3.2.

### ðŸŒŠ VAGUE 2 : CrÃ©ation de Devis (Mix d'Ã‰tats)

**Objectif :** GÃ©nÃ©rer 20 devis avec diffÃ©rents statuts pour tester le filtrage.

```bash
make generate-quotes
```

**Ce qui se passe :**
1. âœ… 20 devis crÃ©Ã©s avec statuts alÃ©atoires :
   - ~12 devis **VALIDATED** (60%)
   - ~4 devis **DRAFT** (20%)
   - ~4 devis **CANCELLED** (20%)

2. âœ… **Producer** â†’ Ã©crit dans `devis-events`

3. âœ… **Kafka Streams** :
   - Filtre â†’ Garde uniquement les VALIDATED (~12)
   - Join â†’ Enrichit avec `product-pricing` (KTable)
   - Transform â†’ Calcule `finalPremium = basePremium * (1 + taxRate)`
   - Ã‰crit dans `all-quotes`

4. âœ… **Kafka Connect** â†’ Synchronise vers Redis

5. âœ… **Query API** â†’ Peut maintenant lire depuis Redis

**Attendre 5 secondes** pour la propagation complÃ¨te.

**VÃ©rifier le filtrage :**
```bash
# Topic d'entrÃ©e : tous les devis (20)
make consume-quotes

# Topic filtrÃ© : devis validÃ©s uniquement (~12)
make consume-validated
```

**VÃ©rifier l'enrichissement :**
```bash
# Devis enrichis avec productName, basePrice, taxRate, finalPremium
make consume-all-quotes
```

Exemple de devis enrichi :
```json
{
  "quoteId": "Q-a3f2b1c4",
  "customerId": "C002",
  "status": "VALIDATED",
  "productCode": "AUTO",
  "basePremium": 450.0,
  "finalPremium": 540.0,      â† CalculÃ©: 450 * 1.20
  "productName": "Auto Insurance",
  "basePrice": 500.0,
  "taxRate": 0.20,
  "createdAt": 1702394821000,
  "updatedAt": 1702394821000
}
```

**VÃ©rifier dans Redis :**
```bash
make redis-cli
> KEYS Q-*
# Devrait afficher ~12 clÃ©s (devis validÃ©s)
> GET Q-a3f2b1c4
# Affiche le JSON complet du devis enrichi
> exit
```

**VÃ©rifier via Query API :**
```bash
# Tous les devis
make query-all

# Statistiques globales
make query-stats
```

RÃ©ponse attendue de `/stats` :
```json
{
  "totalQuotes": 12,
  "totalPremium": 8450.50,
  "averagePremium": 704.21,
  "byProduct": {
    "AUTO": 3,
    "HOME": 2,
    "HEALTH": 4,
    "LIFE": 1,
    "TRAVEL": 2
  }
}
```

---

### ðŸŒŠ VAGUE 3 : Ã‰volution des Devis (Changements d'Ã‰tat)

**Objectif :** Simuler le cycle de vie des devis avec validations et annulations.

#### 3.1 GÃ©nÃ©rer 50 nouveaux devis

```bash
curl -X POST "http://localhost:8081/api/quotes/generate?count=50"
```

**RÃ©sultat :** ~30 validÃ©s, ~10 draft, ~10 cancelled.

**Attendre 5 secondes** puis vÃ©rifier Redis :
```bash
make redis-cli
> DBSIZE
# Devrait afficher ~42 (12 + 30 nouveaux validÃ©s)
> exit
```

#### 3.2 Valider un devis spÃ©cifique

```bash
curl -X POST "http://localhost:8081/api/quotes/Q-12345678/validate"
```

**Ce qui se passe :**
1. âœ… Producer crÃ©e un Ã©vÃ©nement avec `status=VALIDATED`
2. âœ… Streams filtre et enrichit
3. âœ… Kafka Connect Ã©crit dans Redis
4. âœ… Query API peut le lire immÃ©diatement

**VÃ©rifier :**
```bash
curl http://localhost:8084/api/quotes/Q-12345678
```

#### 3.3 Annuler un devis

```bash
curl -X POST "http://localhost:8081/api/quotes/Q-12345678/cancel"
```

**Ce qui se passe :**
1. âœ… Producer crÃ©e un Ã©vÃ©nement avec `status=CANCELLED`
2. âœ… Streams **ignore** car pas VALIDATED
3. âœ… Le devis **reste dans Redis** (ancienne version validÃ©e)

âš ï¸ **Note :** Dans cette version simplifiÃ©e, les annulations ne suppriment pas de Redis. Pour une vraie gestion, il faudrait :
- Soit Ã©crire `null` dans `all-quotes` (Kafka tombstone)
- Soit un processus de nettoyage dans Query API

#### 3.4 VÃ©rifier les agrÃ©gations par client

```bash
# Consommer le topic d'agrÃ©gations
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic quote-aggregates \
  --from-beginning \
  --max-messages 5
```

**RÃ©sultat attendu :** Statistiques par client avec fenÃªtre de 1h :
```json
{
  "customerId": "C001",
  "windowStart": 1702394400000,
  "windowEnd": 1702398000000,
  "count": 8,
  "totalPremium": 4850.75
}
```

#### 3.5 RequÃªtes Query API par client

```bash
# Tous les devis du client C001
make query-customer C=C001

# Tous les devis Auto
make query-product P=AUTO
```

---

## ðŸ“Š PARTIE 5 - Visualisation et Monitoring

### Ã‰tape 5.1 : Kafka UI

Ouvrir **http://localhost:8080**

**Exploration des topics :**
1. Topics â†’ `devis-events` â†’ Messages
   - Observer les diffÃ©rents statuts
2. Topics â†’ `product-pricing` â†’ Messages
   - Observer `cleanup.policy=compact`
   - Voir les clÃ©s (productCode)
3. Topics â†’ `all-quotes` â†’ Messages
   - Uniquement des devis enrichis

**Consumer Groups :**
- `event-processor` â†’ Application Kafka Streams
- Observer le lag (devrait Ãªtre proche de 0)

### Ã‰tape 5.2 : Redis Commander

Ouvrir **http://localhost:8085**

**Exploration :**
- Voir toutes les clÃ©s `Q-*`
- Cliquer sur une clÃ© pour voir le JSON complet
- Observer la structure EnrichedQuote

### Ã‰tape 5.3 : Prometheus

Ouvrir **http://localhost:9090**

**RequÃªtes utiles :**

```promql
# Taux d'envoi Producer
rate(kafka_producer_record_send_total[1m])

# Taux de traitement Streams
rate(kafka_stream_task_process_total[1m])

# Consumer lag
sum(kafka_consumer_fetch_manager_records_lag) by (topic)

# Throughput Broker (bytes/sec)
rate(kafka_server_brokertopicmetrics_bytesin_total[1m])
```

### Ã‰tape 5.4 : Grafana

Ouvrir **http://localhost:3000** (admin/admin)

**Dashboards disponibles :**

1. **Kafka Producer Metrics** (6 panels)
   - Records Sent Rate
   - Request Latency
   - Buffer Memory
   - Batch Size
   - Error & Retry Rate
   - Waiting Threads

2. **Kafka Streams Metrics** (9 panels)
   - Process Rate
   - Commit Latency
   - Consumer Lag par partition
   - Poll Records Average
   - Alive Threads
   - Assigned Partitions
   - State Store Size
   - Poll Latency
   - Punctuate Latency

3. **Kafka Broker Metrics** (11 panels)
   - Network Throughput
   - Messages In Rate
   - Request Latency
   - Under Replicated Partitions
   - Log Size par partition
   - CPU Usage
   - JVM Heap Memory
   - GC Collection Time
   - Thread Count

---

## ðŸŽ¯ PARTIE 6 - Tests de Bout en Bout

### Test 1 : Pipeline ComplÃ¨te

```bash
# 1. CrÃ©er un devis
QUOTE_ID=$(curl -s -X POST http://localhost:8081/api/quotes \
  -H "Content-Type: application/json" \
  -d '{
    "customerId": "C001",
    "status": "DRAFT",
    "productCode": "AUTO",
    "basePremium": 450.0
  }' | jq -r '.quoteId')

# 2. Le devis n'est pas encore dans Redis (DRAFT)
curl http://localhost:8084/api/quotes/$QUOTE_ID
# â†’ 404 Not Found

# 3. Valider le devis
curl -X POST "http://localhost:8081/api/quotes/$QUOTE_ID/validate"

# 4. Attendre 2 secondes
sleep 2

# 5. Le devis est maintenant dans Redis (enrichi)
curl http://localhost:8084/api/quotes/$QUOTE_ID
# â†’ 200 OK avec finalPremium calculÃ©e
```

### Test 2 : GÃ©nÃ©ration Massive

```bash
# GÃ©nÃ©rer 100 devis
make generate-100

# Attendre 10 secondes
sleep 10

# VÃ©rifier les statistiques
make query-stats
```

### Test 3 : Visualiser la Topologie

```bash
make topology-describe
```

Copier la sortie et coller sur : https://zz85.github.io/kafka-streams-viz/

**Vous verrez graphiquement :**
- Source: devis-events
- Source: product-pricing (table)
- Filter: status == VALIDATED
- Join: quotes + pricing
- Sink: all-quotes
- Aggregate: fenÃªtre 1h

---

## ðŸŽ“ PARTIE 7 - Concepts ClÃ©s DÃ©montrÃ©s

### 7.1 Pattern CQRS

| Write Model | Read Model |
|-------------|------------|
| Producer API (8081) | Query API (8084) |
| POST /api/quotes | GET /api/quotes |
| Ã‰crit dans Kafka | Lit depuis Redis |
| OptimisÃ© pour Ã©criture | OptimisÃ© pour lecture |

### 7.2 Kafka Streams Operations

```java
// 1. Filter
quotesStream.filter((k, v) -> v.getStatus() == VALIDATED)

// 2. Join (Stream + Table)
quotesStream.join(pricingTable, (quote, pricing) -> {...})

// 3. Transform
quote.setFinalPremium(basePremium * (1 + taxRate))

// 4. Aggregate (avec fenÃªtre)
.windowedBy(TimeWindows.of(Duration.ofHours(1)))
.aggregate(...)
```

### 7.3 Topic CompactÃ© vs Normal

| Aspect | Normal (devis-events) | CompactÃ© (product-pricing) |
|--------|----------------------|----------------------------|
| Cleanup | delete (par temps) | compact (par clÃ©) |
| Historique | Tous les Ã©vÃ©nements | DerniÃ¨re valeur uniquement |
| Usage | KStream | KTable |
| Exemple | Flux de devis | RÃ©fÃ©rentiel de prix |

### 7.4 Kafka Connect

- **Source Connector** : Externe â†’ Kafka
- **Sink Connector** : Kafka â†’ Externe (Redis dans notre cas)
- **Avantage** : Pas de code, juste configuration JSON

---

## ðŸ§ª PARTIE 8 - ExpÃ©rimentations AvancÃ©es

### ExpÃ©rience 1 : Mettre Ã  Jour un Prix

```bash
curl -X POST http://localhost:8081/api/pricing \
  -H "Content-Type: application/json" \
  -d '{
    "productCode": "AUTO",
    "productName": "Auto Insurance PROMO",
    "basePrice": 400.0,
    "taxRate": 0.18
  }'
```

**RÃ©sultat :**
- âœ… Le topic compactÃ© garde la nouvelle valeur
- âœ… Le KTable se met Ã  jour automatiquement
- âœ… Les prochains devis AUTO auront le nouveau prix

### ExpÃ©rience 2 : Stress Test

```bash
# GÃ©nÃ©rer 500 devis rapidement
for i in {1..5}; do
  curl -X POST "http://localhost:8081/api/quotes/generate?count=100" &
done
wait
```

**Observer dans Grafana :**
- Process Rate augmente
- Consumer Lag monte puis redescend
- JVM Heap Memory utilisÃ©

### ExpÃ©rience 3 : RedÃ©marrer Streams

```bash
# 1. ArrÃªter Kafka Streams (Ctrl+C dans Terminal 2)
# 2. GÃ©nÃ©rer des devis
make generate-quotes
# 3. Relancer Kafka Streams
make run-streams
```

**RÃ©sultat :**
- âœ… Streams rattrape le lag automatiquement
- âœ… Tous les devis sont traitÃ©s
- âœ… Rien n'est perdu (durabilitÃ© Kafka)

---

## ðŸ“š PARTIE 9 - Documentation ComplÃ¨te

### Commandes Make Disponibles

```bash
# Infrastructure
make start           # DÃ©marrer tout
make stop            # ArrÃªter tout
make clean           # Supprimer volumes
make status          # VÃ©rifier statut

# Build & Run
make build           # Compiler Maven
make run-producer    # Lancer Producer
make run-streams     # Lancer Streams
make run-query-api   # Lancer Query API

# DonnÃ©es
make init-pricing    # Init rÃ©fÃ©rentiel
make generate-quotes # 20 devis
make generate-100    # 100 devis

# Topics Kafka
make topics          # Lister topics
make describe-topics # DÃ©tails topics
make consume-quotes  # Consommer devis
make consume-validated # Consommer validÃ©s
make consume-all-quotes # Consommer enrichis

# Query API
make query-all       # GET /quotes
make query-stats     # GET /quotes/stats
make query-customer C=C001 # Par client
make query-product P=AUTO  # Par produit

# Redis
make redis-cli       # Redis CLI
make redis-ui        # Ouvrir Commander

# Kafka Connect
make connect-status  # Statut
make connect-redis   # Configurer

# Monitoring
make prometheus      # Ouvrir Prometheus
make grafana         # Ouvrir Grafana
make topology        # Topologie JSON
make topology-describe # Topologie texte
```

### URLs des Services

| Service | URL | Credentials |
|---------|-----|-------------|
| Producer API | http://localhost:8081 | - |
| Streams API | http://localhost:8082 | - |
| Kafka Connect | http://localhost:8083 | - |
| Query API | http://localhost:8084 | - |
| Kafka UI | http://localhost:8080 | - |
| Redis Commander | http://localhost:8085 | - |
| Prometheus | http://localhost:9090 | - |
| Grafana | http://localhost:3000 | admin/admin |


---

## ðŸ§¹ Nettoyage

```bash
# ArrÃªter proprement
Ctrl+C dans chaque terminal (Producer, Streams, Query API)

# ArrÃªter Docker
make stop

# Nettoyage complet (supprime volumes)
make clean
```

---

## ðŸŽ¯ Points ClÃ©s de l'Exercice

âœ… **Aucun dÃ©veloppement requis** - Tout est prÃ©-implÃ©mentÃ©
âœ… **3 vagues d'alimentation** - Pour voir l'Ã©volution des donnÃ©es
âœ… **Pipeline complÃ¨te** - Du Producer au Query API via Redis
âœ… **Monitoring temps rÃ©el** - Prometheus + Grafana
âœ… **Pattern CQRS** - SÃ©paration write/read
âœ… **Kafka Streams** - Filter, Join, Transform, Aggregate
âœ… **Architecture modulaire** - 4 modules Maven

---

## ðŸŽ‰ FÃ©licitations !

Vous avez maintenant une **plateforme complÃ¨te de traitement de devis d'assurance** avec :
- Kafka Streams pour le traitement temps rÃ©el
- Redis pour le cache de lecture
- Kafka Connect pour la synchronisation
- Monitoring complet avec Prometheus et Grafana
- APIs REST sÃ©parÃ©es (write/read)

**Cette architecture est utilisÃ©e en production dans de nombreuses entreprises !** ðŸš€
